{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cluster.dev - Cloud Infrastructures' Management Tool \u00b6 What is it? \u00b6 Cluster.dev is an open-source tool designed to manage Cloud Native Infrastructures with simple declarative manifests - infrastructure templates. It allows you to describe a whole infrastructure and deploy it with a single tool. The infrastructure templates could be based on Terraform modules, Kubernetes manifests, Shell scripts, Helm charts, Kustomize and ArgoCD/Flux applications, OPA policies etc. Cluster.dev sticks those components together so that you could deploy, test and distribute a whole set of components with pinned versions. Principle Diagram \u00b6 Quick Preview \u00b6 How does it work? \u00b6 With cluster.dev you create or download a predefined template, set the variables, then render and deploy a whole infra set. Capabilities: Re-using all existing Terraform private and public modules and Helm Charts. Applying parallel changes in multiple infrastructures concurrently. Using the same global variables and secrets across different infrastructures, clouds and technologies. Templating anything with Go-template function, even Terraform modules in Helm style templates. Create and manage secrets with SOPS or cloud secret storages. Generate a ready-to-use Terraform code. Features \u00b6 Based on DevOps and SRE best-practices. Simple CI/CD integration. GitOps cluster management and application delivery. Automated provisioning of Kubernetes clusters in AWS, Azure, DO and GCE.","title":"Welcome"},{"location":"#clusterdev-cloud-infrastructures-management-tool","text":"","title":"Cluster.dev - Cloud Infrastructures' Management Tool"},{"location":"#what-is-it","text":"Cluster.dev is an open-source tool designed to manage Cloud Native Infrastructures with simple declarative manifests - infrastructure templates. It allows you to describe a whole infrastructure and deploy it with a single tool. The infrastructure templates could be based on Terraform modules, Kubernetes manifests, Shell scripts, Helm charts, Kustomize and ArgoCD/Flux applications, OPA policies etc. Cluster.dev sticks those components together so that you could deploy, test and distribute a whole set of components with pinned versions.","title":"What is it?"},{"location":"#principle-diagram","text":"","title":"Principle Diagram"},{"location":"#quick-preview","text":"","title":"Quick Preview"},{"location":"#how-does-it-work","text":"With cluster.dev you create or download a predefined template, set the variables, then render and deploy a whole infra set. Capabilities: Re-using all existing Terraform private and public modules and Helm Charts. Applying parallel changes in multiple infrastructures concurrently. Using the same global variables and secrets across different infrastructures, clouds and technologies. Templating anything with Go-template function, even Terraform modules in Helm style templates. Create and manage secrets with SOPS or cloud secret storages. Generate a ready-to-use Terraform code.","title":"How does it work?"},{"location":"#features","text":"Based on DevOps and SRE best-practices. Simple CI/CD integration. GitOps cluster management and application delivery. Automated provisioning of Kubernetes clusters in AWS, Azure, DO and GCE.","title":"Features"},{"location":"DevOpsDays21/","text":"DevOps Days 2021 \u00b6 Hi Guys, I'm Vova from SHALB! In SHALB we build and support a hundreds of infrastructures so we have some outcome and experience that we'd like to share. Problems of the modern Cloud Native infrastructures \u00b6 Multiple technologies needs to be coupled \u00b6 Infrastructure code for complete infra contains a different technologies: Terraform, Helm, Docker, Bash, Ansible, Cloud-Init, CI/CD-scripts, SQL's, GitOps applications, Secrets, etc.. With a bunch of specific DSL'es: yaml, hcl, go-template, json(net). And each with the specific code styles : declarative, imperative, interrogative. With the different diff'ing : two or three way merges. And even using different patching across one tool, like: patchesStrategicMerge, patchesJson6902 in kustomize. So you need to compile all that stuff together to be able spawn a whole infra with one shot. And you need one-shot to be clear that it is fully automated and can be GitOps-ed :)! Even super-powerful tool has own limits \u00b6 So thats why: Terragrunt, Terraspace and Atlantis exist for Terraform. Helmfile, Helm Operator exist form Helm. and Helm exist for K8s yaml :). Its hard to deal with variables and secrets \u00b6 Should be passed between different technologies in sometimes unpredictable sequences. In example you need to set the IAM role arn created by Terraform to Cert-Manager controller deployed with Helm values. Variables should be passed across different infrastructures, even located on different clouds. Imagine you need to obtain DNS Zone from CloudFlare, then set 'NS' records in AWS Route53, and then grant an External-DNS controller which is deployed in on-prem K8s provisioned with Rancher to change this zone in AWS... Secrets that needs to be secured and shared across different team members and teams. Team members sometime leave, or accounts could be compromised and you need completely revoke access from them across a set of infras with one shot. Variables should be decoupled from infrastructure pattern itself and needs a wise sane defaults. If you hardcode variables - its hard to reuse such code. Development and Testing \u00b6 You'd like to maximize reusage of the existing infrastructure patterns: - Terraform modules - Helm Charts - K8s Operators - Dockerfile's Pin versions for all you have in your infra, in example: Pin the aws cli and terraform binary version along with Helm, Prometheus operator version and your private kustomize application. Available solutions \u00b6 So to couple their infrastructure with some 'glue' most of engineers have a several ways: CI/CD sequential appying, ex Jenkins/Gitlab job that deploys infra components one by one. Own bash scripts and Makefiles, that pulls code from different repos and applies using hardcoded sequence. Some of them struggle to write everything with one tech: ex Pulumi(but you need to know how to code in JS, GO, .NET), or Terraform (and you fail) :) Some of them rely on existing API (Kuberenetes) architecture like a Crossplane. We create own tool - cluster.dev or 'cdev' \u00b6 It's Capabilities: Re-using all existing Terraform private and public modules and Helm Charts. Templating anything with Go-template functions, even Terraform modules in Helm-style templates. Applying parallel changes in multiple infrastructures concurrently. Using the same global variables and secrets across different infrastructures, clouds and technologies. Create and manage secrets with Sops or cloud secret storages. Generate a ready to use Terraform code. Short Demo \u00b6","title":"DevOps Days 2021"},{"location":"DevOpsDays21/#devops-days-2021","text":"Hi Guys, I'm Vova from SHALB! In SHALB we build and support a hundreds of infrastructures so we have some outcome and experience that we'd like to share.","title":"DevOps Days 2021"},{"location":"DevOpsDays21/#problems-of-the-modern-cloud-native-infrastructures","text":"","title":"Problems of the modern Cloud Native infrastructures"},{"location":"DevOpsDays21/#multiple-technologies-needs-to-be-coupled","text":"Infrastructure code for complete infra contains a different technologies: Terraform, Helm, Docker, Bash, Ansible, Cloud-Init, CI/CD-scripts, SQL's, GitOps applications, Secrets, etc.. With a bunch of specific DSL'es: yaml, hcl, go-template, json(net). And each with the specific code styles : declarative, imperative, interrogative. With the different diff'ing : two or three way merges. And even using different patching across one tool, like: patchesStrategicMerge, patchesJson6902 in kustomize. So you need to compile all that stuff together to be able spawn a whole infra with one shot. And you need one-shot to be clear that it is fully automated and can be GitOps-ed :)!","title":"Multiple technologies needs to be coupled"},{"location":"DevOpsDays21/#even-super-powerful-tool-has-own-limits","text":"So thats why: Terragrunt, Terraspace and Atlantis exist for Terraform. Helmfile, Helm Operator exist form Helm. and Helm exist for K8s yaml :).","title":"Even super-powerful tool has own limits"},{"location":"DevOpsDays21/#its-hard-to-deal-with-variables-and-secrets","text":"Should be passed between different technologies in sometimes unpredictable sequences. In example you need to set the IAM role arn created by Terraform to Cert-Manager controller deployed with Helm values. Variables should be passed across different infrastructures, even located on different clouds. Imagine you need to obtain DNS Zone from CloudFlare, then set 'NS' records in AWS Route53, and then grant an External-DNS controller which is deployed in on-prem K8s provisioned with Rancher to change this zone in AWS... Secrets that needs to be secured and shared across different team members and teams. Team members sometime leave, or accounts could be compromised and you need completely revoke access from them across a set of infras with one shot. Variables should be decoupled from infrastructure pattern itself and needs a wise sane defaults. If you hardcode variables - its hard to reuse such code.","title":"Its hard to deal with variables and secrets"},{"location":"DevOpsDays21/#development-and-testing","text":"You'd like to maximize reusage of the existing infrastructure patterns: - Terraform modules - Helm Charts - K8s Operators - Dockerfile's Pin versions for all you have in your infra, in example: Pin the aws cli and terraform binary version along with Helm, Prometheus operator version and your private kustomize application.","title":"Development and Testing"},{"location":"DevOpsDays21/#available-solutions","text":"So to couple their infrastructure with some 'glue' most of engineers have a several ways: CI/CD sequential appying, ex Jenkins/Gitlab job that deploys infra components one by one. Own bash scripts and Makefiles, that pulls code from different repos and applies using hardcoded sequence. Some of them struggle to write everything with one tech: ex Pulumi(but you need to know how to code in JS, GO, .NET), or Terraform (and you fail) :) Some of them rely on existing API (Kuberenetes) architecture like a Crossplane.","title":"Available solutions"},{"location":"DevOpsDays21/#we-create-own-tool-clusterdev-or-cdev","text":"It's Capabilities: Re-using all existing Terraform private and public modules and Helm Charts. Templating anything with Go-template functions, even Terraform modules in Helm-style templates. Applying parallel changes in multiple infrastructures concurrently. Using the same global variables and secrets across different infrastructures, clouds and technologies. Create and manage secrets with Sops or cloud secret storages. Generate a ready to use Terraform code.","title":"We create own tool - cluster.dev or 'cdev'"},{"location":"DevOpsDays21/#short-demo","text":"","title":"Short Demo"},{"location":"ROADMAP/","text":"Project Roadmap \u00b6 v.0.1.x - Basic Scenario \u00b6 Create a state storage (AWS S3+Dynamo) for infrastructure resources Deploy a Kubernetes(Minikube) in AWS using default VPC Provision Kubernetes with addons: Ingress-Nginx, Load Balancer, Cert-Manager, ExtDNS, ArgoCD Deploy a sample \"WordPress\" application to Kubernetes cluster using ArgoCD Delivered as GitHub Actions and Docker Image v0.2.x - Bash-based PoC \u00b6 Deliver with cluster creation a default DNS sub-zone: *.username-clustername.cluster.dev Create a cluster.dev backend to register newly created clusters Support for GitLab CI Pipelines ArgoCD sample applications (raw manifests, local helm chart, public helm chart) Support for DigitalOcean Kubernetes cluster 59 DigitalOcean Domains sub-zones 65 AWS EKS provisioning. Spot and Mixed ASG support. Support for Operator Lifecycle Manager v0.3.x - Go-based Beta \u00b6 Go-based reconciler External secrets management with Sops and godaddy/kubernetes-external-secrets Team and user management with Keycloak Apps deployment: Kubernetes Dashboard, Grafana and Kibana. OIDC access to kubeconfig with Keycloak and jetstack/kube-oidc-proxy/ 53 SSO access to ArgoCD and base applications: Kubernetes Dashboard, Grafana, Kibana OIDC integration with GitHub, GitLab, Google Auth, Okta v0.4.x \u00b6 CLI Installer 54 Add GitHub runner and test GitHub Action Continuous Integration workflow Argo Workflows for DAG and CI tasks inside Kubernetes cluster Google Cloud Platform Kubernetes (GKE) support Custom Terraform modules and reconcilation Kind provisioner v0.5.x \u00b6 kops provisioner support k3s provisioner Cost $$$ estimation during installation Web user interface design v0.6.x \u00b6 Rancher RKE provisioner support Multi-cluster support for user management and SSO Multi-cluster support for ArgoCD Crossplane integration","title":"Project Roadmap"},{"location":"ROADMAP/#project-roadmap","text":"","title":"Project Roadmap"},{"location":"ROADMAP/#v01x-basic-scenario","text":"Create a state storage (AWS S3+Dynamo) for infrastructure resources Deploy a Kubernetes(Minikube) in AWS using default VPC Provision Kubernetes with addons: Ingress-Nginx, Load Balancer, Cert-Manager, ExtDNS, ArgoCD Deploy a sample \"WordPress\" application to Kubernetes cluster using ArgoCD Delivered as GitHub Actions and Docker Image","title":"v.0.1.x - Basic Scenario"},{"location":"ROADMAP/#v02x-bash-based-poc","text":"Deliver with cluster creation a default DNS sub-zone: *.username-clustername.cluster.dev Create a cluster.dev backend to register newly created clusters Support for GitLab CI Pipelines ArgoCD sample applications (raw manifests, local helm chart, public helm chart) Support for DigitalOcean Kubernetes cluster 59 DigitalOcean Domains sub-zones 65 AWS EKS provisioning. Spot and Mixed ASG support. Support for Operator Lifecycle Manager","title":"v0.2.x - Bash-based PoC"},{"location":"ROADMAP/#v03x-go-based-beta","text":"Go-based reconciler External secrets management with Sops and godaddy/kubernetes-external-secrets Team and user management with Keycloak Apps deployment: Kubernetes Dashboard, Grafana and Kibana. OIDC access to kubeconfig with Keycloak and jetstack/kube-oidc-proxy/ 53 SSO access to ArgoCD and base applications: Kubernetes Dashboard, Grafana, Kibana OIDC integration with GitHub, GitLab, Google Auth, Okta","title":"v0.3.x - Go-based Beta"},{"location":"ROADMAP/#v04x","text":"CLI Installer 54 Add GitHub runner and test GitHub Action Continuous Integration workflow Argo Workflows for DAG and CI tasks inside Kubernetes cluster Google Cloud Platform Kubernetes (GKE) support Custom Terraform modules and reconcilation Kind provisioner","title":"v0.4.x"},{"location":"ROADMAP/#v05x","text":"kops provisioner support k3s provisioner Cost $$$ estimation during installation Web user interface design","title":"v0.5.x"},{"location":"ROADMAP/#v06x","text":"Rancher RKE provisioner support Multi-cluster support for user management and SSO Multi-cluster support for ArgoCD Crossplane integration","title":"v0.6.x"},{"location":"aws-cloud-provider/","text":"Deploying to AWS \u00b6 cdev uses project templates to generate projects in a desired cloud. This section describes steps necessary to start working with cdev in AWS cloud using AWS-EKS template. Prerequisites to use AWS-EKS template \u00b6 Terraform version 13+. AWS account. AWS CLI installed. kubectl installed. cdev installed . Authentication \u00b6 cdev requires cloud credentials to manage and provision resources. You can configure access to AWS in two ways: Info Please note that you have to use IAM user with granted administrative permissions. Environment variables : provide your credentials via the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY , the environment variables that represent your AWS Access Key and AWS Secret Key. You can also use the AWS_DEFAULT_REGION or AWS_REGION environment variable to set region, if needed. Example usage: export AWS_ACCESS_KEY_ID = \"MYACCESSKEY\" export AWS_SECRET_ACCESS_KEY = \"MYSECRETKEY\" export AWS_DEFAULT_REGION = \"eu-central-1\" Shared Credentials File (recommended) : set up an AWS configuration file to specify your credentials. Credentials file ~/.aws/credentials example: [ cluster-dev ] aws_access_key_id = MYACCESSKEY aws_secret_access_key = MYSECRETKEY Config: ~/.aws/config example: [ profile cluster-dev ] region = eu-central-1 Then export AWS_PROFILE environment variable. export AWS_PROFILE = cluster-dev Install AWS client and check access \u00b6 If you do not have the AWS CLI installed, refer to AWS CLI official installation guide , or use commands from the example: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install aws s3 ls Create S3 bucket for states \u00b6 cdev uses S3 bucket for storing states. Create the bucket with the command: aws s3 mb s3://cdev-states DNS Zone \u00b6 For the AWS-EKS template example, you need to define a Route 53 hosted zone. Options: You already have a Route 53 hosted zone. Create a new hosted zone using a Route 53 documentation example . Use \"cluster.dev\" domain for zone delegation. AWS-EKS \u00b6 AWS-EKS is a cdev template that creates and provisions Kubernetes clusters in AWS cloud by means of Amazon Elastic Kubernetes Service (EKS). AWS-EKS starting guide \u00b6 Configure access to AWS and export required variables. Create locally a project directory, cd into it and execute the command: cdev project create https://github.com/shalb/cdev-aws-eks Tip The template's repo could contain several options for project generation. To list available generators, use --list-templates option: cdev project create https://github.com/shalb/cdev-aws-eks --list-templates Then you can specify which generator to use, for example: cdev project create https://github.com/shalb/cdev-aws-eks minimal Tip If you leave it unspecified, cdev will generate a default project for you. You can also opt for an interactive mode with the extended menu: cdev project create https://github.com/shalb/cdev-aws-eks --interactive Edit variables in the example's files, if necessary: project.yaml - main project config. Sets common global variables for current project such as organization, region, state bucket name etc. See project configuration docs . backend.yaml - configures backend for cdev states (including Terraform states). Uses variables from project.yaml. See backend docs . infra.yaml - describes infrastructure configuration. See infrastructure docs . Run cdev plan to build the project. In the output you will see an infrastructure that is going to be created after running cdev apply . Note Prior to running cdev apply make sure to look through the infra.yaml file and replace the commented fields with real values. In case you would like to use existing VPC and subnets, uncomment preset options and set correct VPC ID and subnets' IDs. If you leave them as is, cdev will have VPC and subnets created for you. Run cdev apply Tip We highly recommend to run cdev apply in a debug mode so that you could see cdev logging in the output: cdev apply -l debug After cdev apply is successfully executed, in the output you will see the ArgoCD URL of your cluster. Sign in to the console to check whether ArgoCD is up and running and the template has been deployed correctly. To sign in, use the \"admin\" login and the bcrypted password that you have generated for the infra.yaml. Displayed in the output will be also a command on how to get kubeconfig and connect to your Kubernetes cluster. Destroy the cluster and all created resources with the command cdev destroy Resources to be created \u00b6 (optional, if you use cluster.dev domain) Route53 zone .cluster.dev (optional, if vpc_id is not set) VPC for EKS cluster EKS Kubernetes cluster with addons: cert-manager ingress-nginx external-dns argocd AWS IAM roles for EKS IRSA cert-manager and external-dns","title":"Deploying to AWS"},{"location":"aws-cloud-provider/#deploying-to-aws","text":"cdev uses project templates to generate projects in a desired cloud. This section describes steps necessary to start working with cdev in AWS cloud using AWS-EKS template.","title":"Deploying to AWS"},{"location":"aws-cloud-provider/#prerequisites-to-use-aws-eks-template","text":"Terraform version 13+. AWS account. AWS CLI installed. kubectl installed. cdev installed .","title":"Prerequisites to use AWS-EKS template"},{"location":"aws-cloud-provider/#authentication","text":"cdev requires cloud credentials to manage and provision resources. You can configure access to AWS in two ways: Info Please note that you have to use IAM user with granted administrative permissions. Environment variables : provide your credentials via the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY , the environment variables that represent your AWS Access Key and AWS Secret Key. You can also use the AWS_DEFAULT_REGION or AWS_REGION environment variable to set region, if needed. Example usage: export AWS_ACCESS_KEY_ID = \"MYACCESSKEY\" export AWS_SECRET_ACCESS_KEY = \"MYSECRETKEY\" export AWS_DEFAULT_REGION = \"eu-central-1\" Shared Credentials File (recommended) : set up an AWS configuration file to specify your credentials. Credentials file ~/.aws/credentials example: [ cluster-dev ] aws_access_key_id = MYACCESSKEY aws_secret_access_key = MYSECRETKEY Config: ~/.aws/config example: [ profile cluster-dev ] region = eu-central-1 Then export AWS_PROFILE environment variable. export AWS_PROFILE = cluster-dev","title":"Authentication"},{"location":"aws-cloud-provider/#install-aws-client-and-check-access","text":"If you do not have the AWS CLI installed, refer to AWS CLI official installation guide , or use commands from the example: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install aws s3 ls","title":"Install AWS client and check access"},{"location":"aws-cloud-provider/#create-s3-bucket-for-states","text":"cdev uses S3 bucket for storing states. Create the bucket with the command: aws s3 mb s3://cdev-states","title":"Create S3 bucket for states"},{"location":"aws-cloud-provider/#dns-zone","text":"For the AWS-EKS template example, you need to define a Route 53 hosted zone. Options: You already have a Route 53 hosted zone. Create a new hosted zone using a Route 53 documentation example . Use \"cluster.dev\" domain for zone delegation.","title":"DNS Zone"},{"location":"aws-cloud-provider/#aws-eks","text":"AWS-EKS is a cdev template that creates and provisions Kubernetes clusters in AWS cloud by means of Amazon Elastic Kubernetes Service (EKS).","title":"AWS-EKS"},{"location":"aws-cloud-provider/#aws-eks-starting-guide","text":"Configure access to AWS and export required variables. Create locally a project directory, cd into it and execute the command: cdev project create https://github.com/shalb/cdev-aws-eks Tip The template's repo could contain several options for project generation. To list available generators, use --list-templates option: cdev project create https://github.com/shalb/cdev-aws-eks --list-templates Then you can specify which generator to use, for example: cdev project create https://github.com/shalb/cdev-aws-eks minimal Tip If you leave it unspecified, cdev will generate a default project for you. You can also opt for an interactive mode with the extended menu: cdev project create https://github.com/shalb/cdev-aws-eks --interactive Edit variables in the example's files, if necessary: project.yaml - main project config. Sets common global variables for current project such as organization, region, state bucket name etc. See project configuration docs . backend.yaml - configures backend for cdev states (including Terraform states). Uses variables from project.yaml. See backend docs . infra.yaml - describes infrastructure configuration. See infrastructure docs . Run cdev plan to build the project. In the output you will see an infrastructure that is going to be created after running cdev apply . Note Prior to running cdev apply make sure to look through the infra.yaml file and replace the commented fields with real values. In case you would like to use existing VPC and subnets, uncomment preset options and set correct VPC ID and subnets' IDs. If you leave them as is, cdev will have VPC and subnets created for you. Run cdev apply Tip We highly recommend to run cdev apply in a debug mode so that you could see cdev logging in the output: cdev apply -l debug After cdev apply is successfully executed, in the output you will see the ArgoCD URL of your cluster. Sign in to the console to check whether ArgoCD is up and running and the template has been deployed correctly. To sign in, use the \"admin\" login and the bcrypted password that you have generated for the infra.yaml. Displayed in the output will be also a command on how to get kubeconfig and connect to your Kubernetes cluster. Destroy the cluster and all created resources with the command cdev destroy","title":"AWS-EKS starting guide"},{"location":"aws-cloud-provider/#resources-to-be-created","text":"(optional, if you use cluster.dev domain) Route53 zone .cluster.dev (optional, if vpc_id is not set) VPC for EKS cluster EKS Kubernetes cluster with addons: cert-manager ingress-nginx external-dns argocd AWS IAM roles for EKS IRSA cert-manager and external-dns","title":"Resources to be created"},{"location":"azure-cloud-provider/","text":"Deploying to Azure \u00b6 Work on setting up access to Azure is in progress, examples are coming soon! Authentication \u00b6 See Terraform Azure provider documentation .","title":"Deploying to Azure"},{"location":"azure-cloud-provider/#deploying-to-azure","text":"Work on setting up access to Azure is in progress, examples are coming soon!","title":"Deploying to Azure"},{"location":"azure-cloud-provider/#authentication","text":"See Terraform Azure provider documentation .","title":"Authentication"},{"location":"cdev-installation-reference/","text":"cdev Install Reference \u00b6 Download from release \u00b6 Each stable version of cdev has a binary that can be downloaded and installed manually. The documentation is suitable for v0.4.0 or higher of cluster.dev client. Installation example for Linux amd64: Download your desired version from the releases page . Unpack it. Find the cdev binary in the unpacked directory. Move the cdev binary to bin folder (/usr/local/bin/). Building from source \u00b6 Go version 16 or higher is required, see Golang installation instructions . To build cluster.dev client from source: Clone cluster.dev Git repo: git clone https://github.com/shalb/cluster.dev/ Build the binary: cd cluster.dev/ && make Check cdev and move the binary to bin folder: ./bin/cdev --help mv ./bin/cdev /usr/local/bin/","title":"cdev Install Reference"},{"location":"cdev-installation-reference/#cdev-install-reference","text":"","title":"cdev Install Reference"},{"location":"cdev-installation-reference/#download-from-release","text":"Each stable version of cdev has a binary that can be downloaded and installed manually. The documentation is suitable for v0.4.0 or higher of cluster.dev client. Installation example for Linux amd64: Download your desired version from the releases page . Unpack it. Find the cdev binary in the unpacked directory. Move the cdev binary to bin folder (/usr/local/bin/).","title":"Download from release"},{"location":"cdev-installation-reference/#building-from-source","text":"Go version 16 or higher is required, see Golang installation instructions . To build cluster.dev client from source: Clone cluster.dev Git repo: git clone https://github.com/shalb/cluster.dev/ Build the binary: cd cluster.dev/ && make Check cdev and move the binary to bin folder: ./bin/cdev --help mv ./bin/cdev /usr/local/bin/","title":"Building from source"},{"location":"cli-commands/","text":"CLI Commands \u00b6 Available commands: apply Apply all modules - build project (see build command), calculate dependencies, create and check graph. Deploy all modules according to the graph. build Build all modules - read project configuration. Check it, render templates and generate code of all modules in tmp dir: ./cluster.dev/project-name/ . destroy Destroy all modules - build project (see build command), calculate dependencies, create and check the reverse graph. Run destroy scenario for all modules according to the graph. help Help about any command. new Code generator. Creates new 'project' or 'secret' from template in an interactive mode. output Display the template's output. plan Plan all modules - build project. Try to run the plan scenario for modules. Modules often refer to the remote states of other modules. Because of this, the plan command may fail if the remote state does not already exist. project Manage projects: info Read project and info message. create Creates new 'project' from template in an interactive mode. secret Manage secrets: ls List secrets in current project. edit Edit secret by name. Usage: cdev secret edit secret-name . create Creates new 'secret' from template in an interactive mode.","title":"CLI Commands"},{"location":"cli-commands/#cli-commands","text":"Available commands: apply Apply all modules - build project (see build command), calculate dependencies, create and check graph. Deploy all modules according to the graph. build Build all modules - read project configuration. Check it, render templates and generate code of all modules in tmp dir: ./cluster.dev/project-name/ . destroy Destroy all modules - build project (see build command), calculate dependencies, create and check the reverse graph. Run destroy scenario for all modules according to the graph. help Help about any command. new Code generator. Creates new 'project' or 'secret' from template in an interactive mode. output Display the template's output. plan Plan all modules - build project. Try to run the plan scenario for modules. Modules often refer to the remote states of other modules. Because of this, the plan command may fail if the remote state does not already exist. project Manage projects: info Read project and info message. create Creates new 'project' from template in an interactive mode. secret Manage secrets: ls List secrets in current project. edit Edit secret by name. Usage: cdev secret edit secret-name . create Creates new 'secret' from template in an interactive mode.","title":"CLI Commands"},{"location":"cli-options/","text":"CLI Options \u00b6 Global flags \u00b6 --cache Use previously cached build directory. -l, --log-level string Set the logging level ('debug'|'info'|'warn'|'error'|'fatal') (default \"info\"). --parallelism int Max parallel threads for module applying (default - 3 ). --trace Print functions trace info in logs (mainly used for development). Apply flags \u00b6 --force Skip interactive approval. -h , --help Help for apply. --ignore-state Apply even if the state has not changed. Create flags \u00b6 -h , --help Help for create. --interactive Use interactive mode for project generation. --list-templates Show all available templates for project generator. Destroy flags \u00b6 --force Skip interactive approval. -h , --help Help for destroy. --ignore-state Destroy current configuration and ignore state.","title":"CLI Options"},{"location":"cli-options/#cli-options","text":"","title":"CLI Options"},{"location":"cli-options/#global-flags","text":"--cache Use previously cached build directory. -l, --log-level string Set the logging level ('debug'|'info'|'warn'|'error'|'fatal') (default \"info\"). --parallelism int Max parallel threads for module applying (default - 3 ). --trace Print functions trace info in logs (mainly used for development).","title":"Global flags"},{"location":"cli-options/#apply-flags","text":"--force Skip interactive approval. -h , --help Help for apply. --ignore-state Apply even if the state has not changed.","title":"Apply flags"},{"location":"cli-options/#create-flags","text":"-h , --help Help for create. --interactive Use interactive mode for project generation. --list-templates Show all available templates for project generator.","title":"Create flags"},{"location":"cli-options/#destroy-flags","text":"--force Skip interactive approval. -h , --help Help for destroy. --ignore-state Destroy current configuration and ignore state.","title":"Destroy flags"},{"location":"digital-ocean-cloud-provider/","text":"Deploying to DigitalOcean \u00b6 cdev uses project templates to generate projects in a desired cloud. This section describes steps necessary to start working with cdev in DigitalOcean cloud using DO-k8s template. Prerequisites to use DO-k8s template \u00b6 Terraform version 13+. DigitalOcean account. doctl installed . cdev installed . Authentication \u00b6 Create an access token for a user. Info Make sure to grant the user with administrative permissions. For details on using DO spaces bucket as a backend, see here . Configure access to DigitalOcean \u00b6 Install doctl . For more information, see the official documentation . cd ~ wget https://github.com/digitalocean/doctl/releases/download/v1.57.0/doctl-1.57.0-linux-amd64.tar.gz tar xf ~/doctl-1.57.0-linux-amd64.tar.gz sudo mv ~/doctl /usr/local/bin Export your DIGITALOCEAN_TOKEN, for details see here . export DIGITALOCEAN_TOKEN = \"MyDIGITALOCEANToken\" Export SPACES_ACCESS_KEY_ID and SPACES_SECRET_ACCESS_KEY environment variables, for details see here . export SPACES_SECRET_ACCESS_KEY = \"dSUGdbJqa6xwJ6Fo8qV2DSksdjh...\" export SPACES_SECRET_ACCESS_KEY = \"TEaKjdj8DSaJl7EnOdsa...\" Create a spaces bucket for Terraform states in the chosen region (in the example we used the 'cdev-data' bucket name). Create a domain in DigitalOcean domains service. Info In the project generated by default we used 'k8s.cluster.dev' zone as an example. Please make sure to change it. DO-k8s \u00b6 DO-k8s is a cdev template that creates and provisions Kubernetes clusters in the DigitalOcean cloud. DO-k8s starting guide \u00b6 Configure access to DigitalOcean and export required variables. Create locally a project directory, cd into it and execute the command: cdev project create https://github.com/shalb/cdev-do-k8s Tip The template's repo could contain several options for project generation. To list available generators, use --list-templates option: cdev project create https://github.com/shalb/cdev-do-k8s --list-templates Then you can specify which generator to use, for example: cdev project create https://github.com/shalb/cdev-do-k8s minimal Tip If you leave it unspecified, cdev will generate a default project for you. You can also opt for an interactive mode with the extended menu: cdev project create https://github.com/shalb/cdev-do-k8s --interactive Edit variables in the example's files, if necessary: project.yaml - main project config. Sets common global variables for current project such as organization, region, state bucket name etc. See project configuration docs . backend.yaml - configures backend for cdev states (including Terraform states). Uses variables from project.yaml. See backend docs . infra.yaml - describes infrastructure configuration. See infrastructure docs . Run cdev plan to build the project. In the output you will see an infrastructure that is going to be created after running cdev apply . Note Prior to running cdev apply make sure to look through the infra.yaml file and replace the commented fields with real values. In case you would like to use existing VPC and subnets, uncomment preset options and set correct VPC ID and subnets' IDs. If you leave them as is, cdev will have VPC and subnets created for you. Run cdev apply Tip We highly recommend to run cdev apply in a debug mode so that you could see cdev logging in the output: cdev apply -l debug After cdev apply is successfully executed, in the output you will see the ArgoCD URL of your cluster. Sign in to the console to check whether ArgoCD is up and running and the template has been deployed correctly. To sign in, use the \"admin\" login and the bcrypted password that you have generated for the infra.yaml. Displayed in the output will be also a command on how to get kubeconfig and connect to your Kubernetes cluster. Destroy the cluster and all created resources with the command cdev destroy Resources to be created \u00b6 (optional, if vpc_id is not set) VPC for Kubernetes cluster DO Kubernetes cluster with addons: cert-manager argocd","title":"Deploying to DigitalOcean"},{"location":"digital-ocean-cloud-provider/#deploying-to-digitalocean","text":"cdev uses project templates to generate projects in a desired cloud. This section describes steps necessary to start working with cdev in DigitalOcean cloud using DO-k8s template.","title":"Deploying to DigitalOcean"},{"location":"digital-ocean-cloud-provider/#prerequisites-to-use-do-k8s-template","text":"Terraform version 13+. DigitalOcean account. doctl installed . cdev installed .","title":"Prerequisites to use DO-k8s template"},{"location":"digital-ocean-cloud-provider/#authentication","text":"Create an access token for a user. Info Make sure to grant the user with administrative permissions. For details on using DO spaces bucket as a backend, see here .","title":"Authentication"},{"location":"digital-ocean-cloud-provider/#configure-access-to-digitalocean","text":"Install doctl . For more information, see the official documentation . cd ~ wget https://github.com/digitalocean/doctl/releases/download/v1.57.0/doctl-1.57.0-linux-amd64.tar.gz tar xf ~/doctl-1.57.0-linux-amd64.tar.gz sudo mv ~/doctl /usr/local/bin Export your DIGITALOCEAN_TOKEN, for details see here . export DIGITALOCEAN_TOKEN = \"MyDIGITALOCEANToken\" Export SPACES_ACCESS_KEY_ID and SPACES_SECRET_ACCESS_KEY environment variables, for details see here . export SPACES_SECRET_ACCESS_KEY = \"dSUGdbJqa6xwJ6Fo8qV2DSksdjh...\" export SPACES_SECRET_ACCESS_KEY = \"TEaKjdj8DSaJl7EnOdsa...\" Create a spaces bucket for Terraform states in the chosen region (in the example we used the 'cdev-data' bucket name). Create a domain in DigitalOcean domains service. Info In the project generated by default we used 'k8s.cluster.dev' zone as an example. Please make sure to change it.","title":"Configure access to DigitalOcean"},{"location":"digital-ocean-cloud-provider/#do-k8s","text":"DO-k8s is a cdev template that creates and provisions Kubernetes clusters in the DigitalOcean cloud.","title":"DO-k8s"},{"location":"digital-ocean-cloud-provider/#do-k8s-starting-guide","text":"Configure access to DigitalOcean and export required variables. Create locally a project directory, cd into it and execute the command: cdev project create https://github.com/shalb/cdev-do-k8s Tip The template's repo could contain several options for project generation. To list available generators, use --list-templates option: cdev project create https://github.com/shalb/cdev-do-k8s --list-templates Then you can specify which generator to use, for example: cdev project create https://github.com/shalb/cdev-do-k8s minimal Tip If you leave it unspecified, cdev will generate a default project for you. You can also opt for an interactive mode with the extended menu: cdev project create https://github.com/shalb/cdev-do-k8s --interactive Edit variables in the example's files, if necessary: project.yaml - main project config. Sets common global variables for current project such as organization, region, state bucket name etc. See project configuration docs . backend.yaml - configures backend for cdev states (including Terraform states). Uses variables from project.yaml. See backend docs . infra.yaml - describes infrastructure configuration. See infrastructure docs . Run cdev plan to build the project. In the output you will see an infrastructure that is going to be created after running cdev apply . Note Prior to running cdev apply make sure to look through the infra.yaml file and replace the commented fields with real values. In case you would like to use existing VPC and subnets, uncomment preset options and set correct VPC ID and subnets' IDs. If you leave them as is, cdev will have VPC and subnets created for you. Run cdev apply Tip We highly recommend to run cdev apply in a debug mode so that you could see cdev logging in the output: cdev apply -l debug After cdev apply is successfully executed, in the output you will see the ArgoCD URL of your cluster. Sign in to the console to check whether ArgoCD is up and running and the template has been deployed correctly. To sign in, use the \"admin\" login and the bcrypted password that you have generated for the infra.yaml. Displayed in the output will be also a command on how to get kubeconfig and connect to your Kubernetes cluster. Destroy the cluster and all created resources with the command cdev destroy","title":"DO-k8s starting guide"},{"location":"digital-ocean-cloud-provider/#resources-to-be-created","text":"(optional, if vpc_id is not set) VPC for Kubernetes cluster DO Kubernetes cluster with addons: cert-manager argocd","title":"Resources to be created"},{"location":"documentation-structure/","text":"Documentation Structure \u00b6 This page explains how the cdev documentation is organized, and is aimed to help you navigate through the cluster.dev website more easily and effectively. Info Please note that cdev documentation is intended for two groups of users and is divided respectively. Infrastructure users - engineers who want to create infrastructures from ready-made templates. No Terraform or Helm knowledge required. Just choose a template, fill in required variables and have your infrastructure running. We provide you with ready-made templates, and metadata dialogs generators. Infrastructure developers - engineers who want to create infrastructure templates of their own, using existing blocks (Terraform modules, Helm charts, k8s manifests) so that other users could create infrastructures based on them. This approach requires strong skills and working experience with Terraform modules and Helm charts development. Website structure \u00b6 Home - introductory section that gives basic knowledge about cdev. Getting Started - contains the list of preliminary conditions needed to start working with cdev, steps on configuring cloud access, and guides on creating projects with cdev templates. Info Is intended for infrastructure users. Template Development - contains description of template blocks, such as functions and modules, and information on creating infrastructure templates (to be added soon). Info Is intended for infrastructure developers. Reference - contains description of project objects, and supplemental information that could be useful both for infrastructure users and developers.","title":"Documentation Structure"},{"location":"documentation-structure/#documentation-structure","text":"This page explains how the cdev documentation is organized, and is aimed to help you navigate through the cluster.dev website more easily and effectively. Info Please note that cdev documentation is intended for two groups of users and is divided respectively. Infrastructure users - engineers who want to create infrastructures from ready-made templates. No Terraform or Helm knowledge required. Just choose a template, fill in required variables and have your infrastructure running. We provide you with ready-made templates, and metadata dialogs generators. Infrastructure developers - engineers who want to create infrastructure templates of their own, using existing blocks (Terraform modules, Helm charts, k8s manifests) so that other users could create infrastructures based on them. This approach requires strong skills and working experience with Terraform modules and Helm charts development.","title":"Documentation Structure"},{"location":"documentation-structure/#website-structure","text":"Home - introductory section that gives basic knowledge about cdev. Getting Started - contains the list of preliminary conditions needed to start working with cdev, steps on configuring cloud access, and guides on creating projects with cdev templates. Info Is intended for infrastructure users. Template Development - contains description of template blocks, such as functions and modules, and information on creating infrastructure templates (to be added soon). Info Is intended for infrastructure developers. Reference - contains description of project objects, and supplemental information that could be useful both for infrastructure users and developers.","title":"Website structure"},{"location":"google-cloud-provider/","text":"Deploying to GCE \u00b6 Work on setting up access to Google Cloud is in progress, examples are coming soon! Authentication \u00b6 See Terraform Google cloud provider documentation .","title":"Deploying to GCE"},{"location":"google-cloud-provider/#deploying-to-gce","text":"Work on setting up access to Google Cloud is in progress, examples are coming soon!","title":"Deploying to GCE"},{"location":"google-cloud-provider/#authentication","text":"See Terraform Google cloud provider documentation .","title":"Authentication"},{"location":"installation/","text":"cdev Install \u00b6 From script recommended \u00b6 cdev has an installer script that takes the latest version of cdev and installs it for you locally. You can fetch the script and execute it locally: curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | bash Tip We recommend installation from script as the easiest and the quickest way to have cdev installed. For other options, please see cdev Install Reference section.","title":"cdev Install"},{"location":"installation/#cdev-install","text":"","title":"cdev Install"},{"location":"installation/#from-script-recommended","text":"cdev has an installer script that takes the latest version of cdev and installs it for you locally. You can fetch the script and execute it locally: curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | bash Tip We recommend installation from script as the easiest and the quickest way to have cdev installed. For other options, please see cdev Install Reference section.","title":"From script recommended"},{"location":"prerequisites/","text":"cdev Prerequisites \u00b6 Supported operation systems: Linux amd64 Darwin amd64 To start using cdev please make sure that you have the following software installed: Git console client Terraform Terraform \u00b6 cdev client uses the Terraform binary. The required Terraform version is ~13 or higher. Refer to the Terraform installation instructions to install Terraform. Terraform installation example for Linux amd64: curl -O https://releases.hashicorp.com/terraform/0.14.7/terraform_0.14.7_linux_amd64.zip unzip terraform_0.14.7_linux_amd64.zip mv terraform /usr/local/bin/","title":"cdev Prerequisites"},{"location":"prerequisites/#cdev-prerequisites","text":"Supported operation systems: Linux amd64 Darwin amd64 To start using cdev please make sure that you have the following software installed: Git console client Terraform","title":"cdev Prerequisites"},{"location":"prerequisites/#terraform","text":"cdev client uses the Terraform binary. The required Terraform version is ~13 or higher. Refer to the Terraform installation instructions to install Terraform. Terraform installation example for Linux amd64: curl -O https://releases.hashicorp.com/terraform/0.14.7/terraform_0.14.7_linux_amd64.zip unzip terraform_0.14.7_linux_amd64.zip mv terraform /usr/local/bin/","title":"Terraform"},{"location":"project-configuration/","text":"Project Configuration \u00b6 Common project files: project.yaml # Contains global project variables that can be used in other configuration objects. <infra_name>.yaml # Contains reference to a template, variables to render a template and backend for states. <backend_name>.yaml # Describes a backend storage for Terraform and cdev states. <secret_name>.yaml # Contains secrets, one per file. cdev reads configuration from current directory, i.e. all files by mask: *.yaml . It is allowed to place several yaml configuration objects in one file, separating them with \"---\". The exception is the project.yaml configuration file and files with secrets. Project represents a single scope for infrastructures within which they are stored and reconciled. The dependencies between different infrastructures can be used within the project scope. Project can host global variables that can be used to template target infrastructure. Project \u00b6 File: project.yaml . Required . Contains global project variables that can be used in other configuration objects, such as backend or infrastructure (except of secrets ). Note that the project.conf file is not rendered with the template and you cannot use template units in it. Example project.yaml : name : my_project kind : project variables : organization : shalb region : eu-central-1 state_bucket_name : cdev-states name : project name. Required . kind : object kind. Must be project . Required . variables : a set of data in yaml format that can be referenced in other configuration objects. For the example above, the link to the organization name will look like this: {{ .project.variables.organization }} . Infrastructure \u00b6 File: searching in ./*.yaml . Required at least one . Infrastructure object ( kind: infrastructure ) contains reference to a template, variables to render the template and backend for states. Example: # Define infrastructure itself name : k3s-infra template : \"./templates/\" kind : infrastructure backend : aws-backend variables : bucket : {{ .project.variables.state_bucket_name }} # Using project variables. region : {{ .project.variables.region }} organization : {{ .project.variables.organization }} domain : cluster.dev instance_type : \"t3.medium\" vpc_id : \"vpc-5ecf1234\" name : infrastructure name. Required . kind : object kind. infrastructure . Required . backend : name of the backend that will be used to store the states of this infrastructure. Required . variables : data set for template rendering. template : it's either a path to a local directory containing the template's configuration files, or a remote Git repository as a template source. For more details on templates please see the Template Development section. A local path must begin with either / for absolute path, ./ or ../ for relative path. For Git source, use this format: <GIT_URL>//<PATH_TO_TEMPLATE_DIR>?ref=<BRANCH_OR_TAG> : <GIT_URL> - required . Standard Git repo url. See details on official Git page . <PATH_TO_TEMPLATE_DIR> - optional , use it if template configuration is not in root of repo. <BRANCH_OR_TAG> - Git branch or tag. Examples: template : /path/to/dir # absolute local path template : ./template/ # relative local path template : ../../template/ # relative local path template : https://github.com/shalb/cdev-k8s # https Git url template : https://github.com/shalb/cdev-k8s//some/dir/ # subdirectory template : https://github.com/shalb/cdev-k8s//some/dir/?ref=branch-name # branch template : https://github.com/shalb/cdev-k8s?ref=v1.1.1 # tag template : git@github.com:shalb/cdev-k8s.git # ssh Git url template : git@github.com:shalb/cdev-k8s.git//some/dir/ # subdirectory template : git@github.com:shalb/cdev-k8s.git//some/dir/?ref=branch-name # branch template : git@github.com:shalb/cdev-k8s.git?ref=v1.1.1 # tag Backends \u00b6 File: searching in ./*.yaml . Required at least one . An object that describes a backend storage for Terraform and cdev states. In the backends' configuration you can use any options of appropriate Terraform backend. They will be converted as is. Currently 4 types of backends are supported: s3 AWS S3 backend: name : aws-backend kind : backend provider : s3 spec : bucket : cdev-states region : {{ .project.variables.region }} do DigitalOcean spaces backend: name : do-backend kind : backend provider : do spec : bucket : cdev-states region : {{ .project.variables.region }} access_key : {{ env \"SPACES_ACCESS_KEY_ID\" }} secret_key : {{ env \"SPACES_SECRET_ACCESS_KEY\" }} azurerm Microsoft azurerm: name : gcs-b kind : backend provider : azurerm spec : resource_group_name : \"StorageAccount-ResourceGroup\" storage_account_name : \"example\" container_name : \"cdev-states\" gcs Google Cloud backend: name : do-backend kind : backend provider : gcs spec : bucket : cdev-states prefix : pref Secrets \u00b6 There are two ways to use secrets: SOPS secrets \u00b6 For creating and editing SOPS secrets, cdev uses SOPS binary. But the SOPS binary is not required for decrypting and using SOPS secrets. As none of cdev reconcilation processes (build, plan, apply) requires SOPS to be performed, you don't have to install it for pipelines. See SOPS installation instructions in official repo. Secrets are encoded/decoded with SOPS utility that supports AWS KMS, GCP KMS, Azure Key Vault and PGP keys. How to use: Use console client cdev to create a new secret from scratch: cdev secret create Use interactive menu to create a secret. Edit the secret and set secret data in encrypted_data: section. Use references to the secret's data in infrastructure template (you can find the examples in the generated secret file). Amazon secret manager \u00b6 cdev client can use AWS SSM as a secret storage. How to use: Create a new secret in AWS secret manager using AWS CLI or web console. Both raw and JSON data formats are supported. Use the console client cdev to create a new secret from scratch: cdev secret create Answer the questions. For Name of secret in AWS Secrets manager enter the name of the AWS secret created above. Use references to the secret's data in infrastructure template (you can find the examples in the generated secret file). To list and edit any secret, use the commands: cdev secret ls and cdev secret edit secret_name Templates \u00b6 Currently there are 3 types of templates available: aws-k3s aws-eks do-k8s For the detailed information on templates, please see the section Template Development .","title":"Project Configuration"},{"location":"project-configuration/#project-configuration","text":"Common project files: project.yaml # Contains global project variables that can be used in other configuration objects. <infra_name>.yaml # Contains reference to a template, variables to render a template and backend for states. <backend_name>.yaml # Describes a backend storage for Terraform and cdev states. <secret_name>.yaml # Contains secrets, one per file. cdev reads configuration from current directory, i.e. all files by mask: *.yaml . It is allowed to place several yaml configuration objects in one file, separating them with \"---\". The exception is the project.yaml configuration file and files with secrets. Project represents a single scope for infrastructures within which they are stored and reconciled. The dependencies between different infrastructures can be used within the project scope. Project can host global variables that can be used to template target infrastructure.","title":"Project Configuration"},{"location":"project-configuration/#project","text":"File: project.yaml . Required . Contains global project variables that can be used in other configuration objects, such as backend or infrastructure (except of secrets ). Note that the project.conf file is not rendered with the template and you cannot use template units in it. Example project.yaml : name : my_project kind : project variables : organization : shalb region : eu-central-1 state_bucket_name : cdev-states name : project name. Required . kind : object kind. Must be project . Required . variables : a set of data in yaml format that can be referenced in other configuration objects. For the example above, the link to the organization name will look like this: {{ .project.variables.organization }} .","title":"Project"},{"location":"project-configuration/#infrastructure","text":"File: searching in ./*.yaml . Required at least one . Infrastructure object ( kind: infrastructure ) contains reference to a template, variables to render the template and backend for states. Example: # Define infrastructure itself name : k3s-infra template : \"./templates/\" kind : infrastructure backend : aws-backend variables : bucket : {{ .project.variables.state_bucket_name }} # Using project variables. region : {{ .project.variables.region }} organization : {{ .project.variables.organization }} domain : cluster.dev instance_type : \"t3.medium\" vpc_id : \"vpc-5ecf1234\" name : infrastructure name. Required . kind : object kind. infrastructure . Required . backend : name of the backend that will be used to store the states of this infrastructure. Required . variables : data set for template rendering. template : it's either a path to a local directory containing the template's configuration files, or a remote Git repository as a template source. For more details on templates please see the Template Development section. A local path must begin with either / for absolute path, ./ or ../ for relative path. For Git source, use this format: <GIT_URL>//<PATH_TO_TEMPLATE_DIR>?ref=<BRANCH_OR_TAG> : <GIT_URL> - required . Standard Git repo url. See details on official Git page . <PATH_TO_TEMPLATE_DIR> - optional , use it if template configuration is not in root of repo. <BRANCH_OR_TAG> - Git branch or tag. Examples: template : /path/to/dir # absolute local path template : ./template/ # relative local path template : ../../template/ # relative local path template : https://github.com/shalb/cdev-k8s # https Git url template : https://github.com/shalb/cdev-k8s//some/dir/ # subdirectory template : https://github.com/shalb/cdev-k8s//some/dir/?ref=branch-name # branch template : https://github.com/shalb/cdev-k8s?ref=v1.1.1 # tag template : git@github.com:shalb/cdev-k8s.git # ssh Git url template : git@github.com:shalb/cdev-k8s.git//some/dir/ # subdirectory template : git@github.com:shalb/cdev-k8s.git//some/dir/?ref=branch-name # branch template : git@github.com:shalb/cdev-k8s.git?ref=v1.1.1 # tag","title":"Infrastructure"},{"location":"project-configuration/#backends","text":"File: searching in ./*.yaml . Required at least one . An object that describes a backend storage for Terraform and cdev states. In the backends' configuration you can use any options of appropriate Terraform backend. They will be converted as is. Currently 4 types of backends are supported: s3 AWS S3 backend: name : aws-backend kind : backend provider : s3 spec : bucket : cdev-states region : {{ .project.variables.region }} do DigitalOcean spaces backend: name : do-backend kind : backend provider : do spec : bucket : cdev-states region : {{ .project.variables.region }} access_key : {{ env \"SPACES_ACCESS_KEY_ID\" }} secret_key : {{ env \"SPACES_SECRET_ACCESS_KEY\" }} azurerm Microsoft azurerm: name : gcs-b kind : backend provider : azurerm spec : resource_group_name : \"StorageAccount-ResourceGroup\" storage_account_name : \"example\" container_name : \"cdev-states\" gcs Google Cloud backend: name : do-backend kind : backend provider : gcs spec : bucket : cdev-states prefix : pref","title":"Backends"},{"location":"project-configuration/#secrets","text":"There are two ways to use secrets:","title":"Secrets"},{"location":"project-configuration/#sops-secrets","text":"For creating and editing SOPS secrets, cdev uses SOPS binary. But the SOPS binary is not required for decrypting and using SOPS secrets. As none of cdev reconcilation processes (build, plan, apply) requires SOPS to be performed, you don't have to install it for pipelines. See SOPS installation instructions in official repo. Secrets are encoded/decoded with SOPS utility that supports AWS KMS, GCP KMS, Azure Key Vault and PGP keys. How to use: Use console client cdev to create a new secret from scratch: cdev secret create Use interactive menu to create a secret. Edit the secret and set secret data in encrypted_data: section. Use references to the secret's data in infrastructure template (you can find the examples in the generated secret file).","title":"SOPS secrets"},{"location":"project-configuration/#amazon-secret-manager","text":"cdev client can use AWS SSM as a secret storage. How to use: Create a new secret in AWS secret manager using AWS CLI or web console. Both raw and JSON data formats are supported. Use the console client cdev to create a new secret from scratch: cdev secret create Answer the questions. For Name of secret in AWS Secrets manager enter the name of the AWS secret created above. Use references to the secret's data in infrastructure template (you can find the examples in the generated secret file). To list and edit any secret, use the commands: cdev secret ls and cdev secret edit secret_name","title":"Amazon secret manager"},{"location":"project-configuration/#templates","text":"Currently there are 3 types of templates available: aws-k3s aws-eks do-k8s For the detailed information on templates, please see the section Template Development .","title":"Templates"},{"location":"style-guide/","text":"Style guide \u00b6 For better experience, we recommend using VS Code - we have a list of recommended extensions to prevent many common errors, improve code and save time. We use .editorconfig . It fixes basic mistakes on every file saving. Please make sure to install pre-commit-terraform with all its dependencies. It checks all changed files when you run git commit for more complex problems and tries to fix them for you. Bash \u00b6 Firstly, please install shellcheck to have vscode-shellcheck extension working properly. We use Google Style Guide . Terraform \u00b6 We use Terraform Best Practices.com code style and conceptions. Autogenerated Documentation \u00b6 For the successful module documentation initialization, you need to create README.md with: <!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK --> <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK --> It is needed for terraform-docs hooks. The hook rewrites all the things inside with every .tf file change. Then run pre-commit run --all-files or make some changes in any .tf file in the same dir (for ex. variable \"name\" { -> variable \"name\"{ ).","title":"Style guide"},{"location":"style-guide/#style-guide","text":"For better experience, we recommend using VS Code - we have a list of recommended extensions to prevent many common errors, improve code and save time. We use .editorconfig . It fixes basic mistakes on every file saving. Please make sure to install pre-commit-terraform with all its dependencies. It checks all changed files when you run git commit for more complex problems and tries to fix them for you.","title":"Style guide"},{"location":"style-guide/#bash","text":"Firstly, please install shellcheck to have vscode-shellcheck extension working properly. We use Google Style Guide .","title":"Bash"},{"location":"style-guide/#terraform","text":"We use Terraform Best Practices.com code style and conceptions.","title":"Terraform"},{"location":"style-guide/#autogenerated-documentation","text":"For the successful module documentation initialization, you need to create README.md with: <!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK --> <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK --> It is needed for terraform-docs hooks. The hook rewrites all the things inside with every .tf file change. Then run pre-commit run --all-files or make some changes in any .tf file in the same dir (for ex. variable \"name\" { -> variable \"name\"{ ).","title":"Autogenerated Documentation"},{"location":"template-development/","text":"Template Development \u00b6 Basics \u00b6 A template is a directory, either local or located in a Git repo that contains template config files. cdev reads all ./*.yaml files from the directory (non-recursively), renders a template with the project's data, parse the yaml file and loads modules. Modules may contain reference to other files that are required for work. These files should be located inside the current directory (template context). As some of the files will also be rendered with the project's data, you can use Go-templates in them. For more details please see modules configuration below. Template represents a yaml structure with an array of different invocation modules. Common view: modules : - module1 - module2 - module3 ... Template can utilize all kinds of Go-templates and Sprig functions (similar to Helm). Along with that it is enhanced with functions like insertYAML that could pass yaml blocks directly. Functions \u00b6 1) Base Go-template language functions . 2) Sprig functions . 3) Enhanced functions: all functions described above allow you to modify the template text. Apart from these, some special enhanced functions are available. They cannot be used everywhere. The functions are integrated within the functionality of the program and with the yaml syntax: insertYAML - pass yaml block as value of target yaml template. Argument : data to pass, any value or reference to block. Allowed use : only as full yaml value, in module inputs . Example: Source yaml: values : node_groups : - name : ng1 min_size : 1 max_size : 5 - name : ng2 max_size : 2 type : spot Target yaml template: modules : - name : k3s type : terraform node_groups : {{ insertYAML .values.node_groups }} Rendered template: modules : - name : k3s type : terraform node_groups : - name : ng1 min_size : 1 max_size : 5 - name : ng2 max_size : 2 type : spot remoteState - is used for passing data between modules and infrastructures, can be used in pre/post hooks. Argument : string, path to remote state consisting of 3 parts separated by a dot: \"infra_name.module_name.output_name\" . Since the name of the infrastructure is unknown inside the template, you can use \"this\" instead: \"this.module_name.output_name\" . Allowed use : all modules types: in inputs ; all modules types: in modules pre/post hooks; in Kubernetes modules: in Kubernetes manifests. cidrSubnet - calculates a subnet address within given IP network address prefix. Same as Terraform function . Example: Source: {{ cidrSubnet \"172.16.0.0/12\" 4 2 }} Rendered: 172 .18.0.0/16 Modules \u00b6 All modules described below have a common format and common fields. Base example: - name : k3s type : terraform depends_on : - this.module1_name - this.module2_name # depends_on: this.module1_name # is allowed to use string for single, or list for multiple dependencies pre_hook : command : \"echo pre_hook\" # script: \"./scripts/hook.sh\" on_apply : true on_destroy : false on_plan : false post_hook : # command: \"echo post_hook\" script : \"./scripts/hook.sh\" on_apply : true on_destroy : false on_plan : false name - module name. Required . type - module type. One of: terraform , helm , kubernetes , printer . See below. depends_on - string or list of strings . One or multiple module dependencies in the format \"infra_name.module_name\". Since the name of the infrastructure is unknown inside the template, you can use \"this\" instead: \"this.module_name.output_name\" . pre_hook and post_hook blocks: describe the shell commands to be executed before and after the module, respectively. The commands will be executed in the same context as the actions of the module. Environment variables are common to the shell commands, the pre_hook and post_hook scripts, and the module execution. You can export a variable in the pre_hook and it will be available in the post_hook or in the module. command - string . Shell command in text format. Will be executed in bash -c \"command\". Can be used if the \"script\" option is not used. One of command or script is required. script - string path to shell script file which is relative to template directory. Can be used if the \"command\" option is not used. One of command or script is required. on_apply bool , optional turn off/on when module applying. Default: \"true\" . on_destroy - bool , optional turn off/on when module destroying. Default: \"false\" . on_plan - bool , optional turn off/on when module plan executing. Default: \"false\" . Terraform module \u00b6 Describes direct Terraform module invocation. Example: modules : - name : vpc type : terraform version : \"2.77.0\" source : terraform-aws-modules/vpc/aws inputs : name : {{ .name }} azs : {{ insertYAML .variables.azs }} vpc_id : {{ .variables.vpc_id }} In addition to common options the following are available: source - string , required . Terraform module source . It is not allowed to use local folders in source! version - string , optional . Module version . inputs - map of any , required . A map that corresponds to input variables defined by the module. This block allows to use functions remoteState and insertYAML . Helm module \u00b6 Describes Terraform Helm provider invocation. Example: modules : - name : argocd type : helm provider_version : \"2.0.3\" source : repository : \"https://argoproj.github.io/argo-helm\" chart : \"argo-cd\" version : \"2.11.0\" kubeconfig : ../kubeconfig depends_on : this.k3s pre_hook : script : ./scripts/get_kubeconfig.sh ./kubeconfig on_destroy : true on_plan : true additional_options : namespace : \"argocd\" create_namespace : true inputs : global.image.tag : v1.8.3 service.type : LoadBalancer In addition to common options the following are available: source - map , required . Block describes Helm chart source. chart , repository , version - correspond to options with the same name from helm_release resource. See chart , repository and version . kubeconfig - string , required . Path to the kubeconfig file which is relative to the directory where the module was executed. provider_version - string , optional . Version of terraform helm provider to use. Default - latest. See terraform helm provider additional_options - map of any , optional . Corresponds to Terraform helm_release resource options . Will be passed as is. inputs - map of any , optional . A map that represents Terraform helm_release sets . This block allows to use functions remoteState and insertYAML . For example: yaml inputs: global.image.tag: v1.8.3 service.type: LoadBalancer corresponds to: ```hcl set { name = \"global.image.tag\" value = \"v1.8.3\" } set { name = \"service.type\" value = \"LoadBalancer\" } ``` Kubernetes module \u00b6 Describes Terraform kubernetes-alpha provider invocation. Example: modules : - name : argocd_apps type : kubernetes provider_version : \"0.2.1\" source : ./argocd-apps/app1.yaml kubeconfig : ../kubeconfig depends_on : this.argocd source - string , required . Path to Kubernetes manifest that will be converted into a representation of kubernetes-alpha provider. Source file will be rendered with the template, and also allows to use the functions remoteState and insertYAML . kubeconfig - string , required . Path to the kubeconfig file which is relative to the directory where the module was executed. provider_version - string , optional . Version of terraform kubernetes-alpha provider to use. Default - latest. See terraform kubernetes-alpha provider Printer module \u00b6 The module is mainly used to see the outputs of other modules in the console logs. Example: modules : - name : print_outputs type : printer inputs : cluster_name : {{ .name }} worker_iam_role_arn : {{ remoteState \"this.eks.worker_iam_role_arn\" }} inputs - any , required - a map that represents data to be printed in the log. The block allows to use the functions remoteState and insertYAML .","title":"Template Development"},{"location":"template-development/#template-development","text":"","title":"Template Development"},{"location":"template-development/#basics","text":"A template is a directory, either local or located in a Git repo that contains template config files. cdev reads all ./*.yaml files from the directory (non-recursively), renders a template with the project's data, parse the yaml file and loads modules. Modules may contain reference to other files that are required for work. These files should be located inside the current directory (template context). As some of the files will also be rendered with the project's data, you can use Go-templates in them. For more details please see modules configuration below. Template represents a yaml structure with an array of different invocation modules. Common view: modules : - module1 - module2 - module3 ... Template can utilize all kinds of Go-templates and Sprig functions (similar to Helm). Along with that it is enhanced with functions like insertYAML that could pass yaml blocks directly.","title":"Basics"},{"location":"template-development/#functions","text":"1) Base Go-template language functions . 2) Sprig functions . 3) Enhanced functions: all functions described above allow you to modify the template text. Apart from these, some special enhanced functions are available. They cannot be used everywhere. The functions are integrated within the functionality of the program and with the yaml syntax: insertYAML - pass yaml block as value of target yaml template. Argument : data to pass, any value or reference to block. Allowed use : only as full yaml value, in module inputs . Example: Source yaml: values : node_groups : - name : ng1 min_size : 1 max_size : 5 - name : ng2 max_size : 2 type : spot Target yaml template: modules : - name : k3s type : terraform node_groups : {{ insertYAML .values.node_groups }} Rendered template: modules : - name : k3s type : terraform node_groups : - name : ng1 min_size : 1 max_size : 5 - name : ng2 max_size : 2 type : spot remoteState - is used for passing data between modules and infrastructures, can be used in pre/post hooks. Argument : string, path to remote state consisting of 3 parts separated by a dot: \"infra_name.module_name.output_name\" . Since the name of the infrastructure is unknown inside the template, you can use \"this\" instead: \"this.module_name.output_name\" . Allowed use : all modules types: in inputs ; all modules types: in modules pre/post hooks; in Kubernetes modules: in Kubernetes manifests. cidrSubnet - calculates a subnet address within given IP network address prefix. Same as Terraform function . Example: Source: {{ cidrSubnet \"172.16.0.0/12\" 4 2 }} Rendered: 172 .18.0.0/16","title":"Functions"},{"location":"template-development/#modules","text":"All modules described below have a common format and common fields. Base example: - name : k3s type : terraform depends_on : - this.module1_name - this.module2_name # depends_on: this.module1_name # is allowed to use string for single, or list for multiple dependencies pre_hook : command : \"echo pre_hook\" # script: \"./scripts/hook.sh\" on_apply : true on_destroy : false on_plan : false post_hook : # command: \"echo post_hook\" script : \"./scripts/hook.sh\" on_apply : true on_destroy : false on_plan : false name - module name. Required . type - module type. One of: terraform , helm , kubernetes , printer . See below. depends_on - string or list of strings . One or multiple module dependencies in the format \"infra_name.module_name\". Since the name of the infrastructure is unknown inside the template, you can use \"this\" instead: \"this.module_name.output_name\" . pre_hook and post_hook blocks: describe the shell commands to be executed before and after the module, respectively. The commands will be executed in the same context as the actions of the module. Environment variables are common to the shell commands, the pre_hook and post_hook scripts, and the module execution. You can export a variable in the pre_hook and it will be available in the post_hook or in the module. command - string . Shell command in text format. Will be executed in bash -c \"command\". Can be used if the \"script\" option is not used. One of command or script is required. script - string path to shell script file which is relative to template directory. Can be used if the \"command\" option is not used. One of command or script is required. on_apply bool , optional turn off/on when module applying. Default: \"true\" . on_destroy - bool , optional turn off/on when module destroying. Default: \"false\" . on_plan - bool , optional turn off/on when module plan executing. Default: \"false\" .","title":"Modules"},{"location":"template-development/#terraform-module","text":"Describes direct Terraform module invocation. Example: modules : - name : vpc type : terraform version : \"2.77.0\" source : terraform-aws-modules/vpc/aws inputs : name : {{ .name }} azs : {{ insertYAML .variables.azs }} vpc_id : {{ .variables.vpc_id }} In addition to common options the following are available: source - string , required . Terraform module source . It is not allowed to use local folders in source! version - string , optional . Module version . inputs - map of any , required . A map that corresponds to input variables defined by the module. This block allows to use functions remoteState and insertYAML .","title":"Terraform module"},{"location":"template-development/#helm-module","text":"Describes Terraform Helm provider invocation. Example: modules : - name : argocd type : helm provider_version : \"2.0.3\" source : repository : \"https://argoproj.github.io/argo-helm\" chart : \"argo-cd\" version : \"2.11.0\" kubeconfig : ../kubeconfig depends_on : this.k3s pre_hook : script : ./scripts/get_kubeconfig.sh ./kubeconfig on_destroy : true on_plan : true additional_options : namespace : \"argocd\" create_namespace : true inputs : global.image.tag : v1.8.3 service.type : LoadBalancer In addition to common options the following are available: source - map , required . Block describes Helm chart source. chart , repository , version - correspond to options with the same name from helm_release resource. See chart , repository and version . kubeconfig - string , required . Path to the kubeconfig file which is relative to the directory where the module was executed. provider_version - string , optional . Version of terraform helm provider to use. Default - latest. See terraform helm provider additional_options - map of any , optional . Corresponds to Terraform helm_release resource options . Will be passed as is. inputs - map of any , optional . A map that represents Terraform helm_release sets . This block allows to use functions remoteState and insertYAML . For example: yaml inputs: global.image.tag: v1.8.3 service.type: LoadBalancer corresponds to: ```hcl set { name = \"global.image.tag\" value = \"v1.8.3\" } set { name = \"service.type\" value = \"LoadBalancer\" } ```","title":"Helm module"},{"location":"template-development/#kubernetes-module","text":"Describes Terraform kubernetes-alpha provider invocation. Example: modules : - name : argocd_apps type : kubernetes provider_version : \"0.2.1\" source : ./argocd-apps/app1.yaml kubeconfig : ../kubeconfig depends_on : this.argocd source - string , required . Path to Kubernetes manifest that will be converted into a representation of kubernetes-alpha provider. Source file will be rendered with the template, and also allows to use the functions remoteState and insertYAML . kubeconfig - string , required . Path to the kubeconfig file which is relative to the directory where the module was executed. provider_version - string , optional . Version of terraform kubernetes-alpha provider to use. Default - latest. See terraform kubernetes-alpha provider","title":"Kubernetes module"},{"location":"template-development/#printer-module","text":"The module is mainly used to see the outputs of other modules in the console logs. Example: modules : - name : print_outputs type : printer inputs : cluster_name : {{ .name }} worker_iam_role_arn : {{ remoteState \"this.eks.worker_iam_role_arn\" }} inputs - any , required - a map that represents data to be printed in the log. The block allows to use the functions remoteState and insertYAML .","title":"Printer module"}]}